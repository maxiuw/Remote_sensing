{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input --> nodes, edges through the activation function, output f(x,theta)\n",
    "- tanh instad of simgoid - we want to divide on + and -\n",
    "- relu - we want derivative near 0 not to be huge but constant \n",
    "\n",
    "- sigmoid - kind of probability / 1- probability\n",
    "\n",
    "<img src=\"img/week71.png\" style=\"width:800px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probability\n",
    "A za\n",
    "B zb \n",
    "c zc\n",
    "\n",
    "e^zA/sum(e^zi)\n",
    "\n",
    "### decision boundary \n",
    "- train but use argmax as uptup\n",
    "- decision budary can be set differntly depending on the function\n",
    "- if u add the functions together we will get nonlinear boundaries \n",
    "<img src=\"img/week72.png\" style=\"width:800px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have input later, ususally fully connected with the neurons (hidden layer)\n",
    "- laten variables \n",
    "- also gradien is connected \n",
    "- using 'chain rule' \n",
    "- any small error, due to the propagation, it had really big influence \n",
    "- with sigmoid, pretty unstable, with relu - much stabler \n",
    "\n",
    "- if we just have linear function, we can just separate with linear line, with nonlinear,we can go and make nonlinear boundaries, \n",
    "\n",
    "- what can happen is overfitting,\n",
    "- more capacity ---> require more data, not to overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU - general purpose computing, since the graphic computing is usually based on matrix multiplication\n",
    "and NN are based on the same thing, then\n",
    "\n",
    "<img src=\"img/week75.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week76.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week77.png\" style=\"width:800px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in pytorch\n",
    "\n",
    "class MyNet(nn.Nodel):\n",
    "    def __init__(self):\n",
    "        super(...)\n",
    "        self.layer1 = torch.nn.linear(input = 100, output = 50)\n",
    "        self.later2 = torch.nn.linear(input = output of lay1, output = x)\n",
    "        \n",
    "    def forward(x):\n",
    "        x = self.layer1(x)\n",
    "        x = relu(x)\n",
    "        ... for all layers in nn\n",
    "        # tensorboard -> 'visualizing nn in toarch', hardly interpetable \n",
    "        \n",
    "    net = MyNet()\n",
    "    print(net) -> really useful visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we usually traing ds\n",
    "ds = Dataset\n",
    "learder = load(ds,pin the memory !!! easy to do in pytorch !!!)\n",
    "for epoch in range(x):\n",
    "    for data in loader:\n",
    "        x,y = data (each value is a tensor)\n",
    "        train!!!\n",
    "        x = move to device\n",
    "        y = move to device \n",
    "        z = net(x)\n",
    "        loss = corret(x) (XNet) -> tensor\n",
    "        loss.backward() -> correcting the weights, \n",
    "        moves back using chain rule, calculating derivatives \n",
    "        \n",
    "    \n",
    "It is expensive to train on GPU and CPU, we have to move our DS to GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset\n",
    "loader = DataLoarder \n",
    "criteria = nn.Xnet()\n",
    "optim = torch.optim.SGD - optimizing weights of networt\n",
    "for ep in range(n_epoch):\n",
    "    for (x,y) in loader:\n",
    "        #update W = W -learning_rate * gradient\n",
    "        # stochastic grad descent, we sum just over some of the elements (e.g. 5), saying \n",
    "        # gradient is equal around this \n",
    "        optim.zero_grad ()\n",
    "        x.to(cuda)\n",
    "        y.to(cuda)\n",
    "        loss = criteria(z,y)\n",
    "        loss.backwards()\n",
    "        optim.update\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batch \n",
    "<img src=\"img/week78.png\" style=\"width:300px;height:400px;\">\n",
    "\n",
    "\n",
    "- segmentation\n",
    "    - unsupervised way to cluster \n",
    "    - due to lighting, it can divide the groups on the smaller groups e.g. tree into 2 groups \n",
    "    - what can we do? reduce number of groups in the image \n",
    "    \n",
    "- semantic segmentatiton \n",
    "    - given the same image + labels are given \n",
    "    - deeplab3 -> general purpose semantic segmentation \n",
    "    - we can feed clustered data and train \n",
    "- plenoptic segmentation\n",
    "    - classifing and clustering \n",
    "    - solved usually by maskRCNN\n",
    "- inscance segmentation/\n",
    "    - input but no lables, kinda clustering\n",
    "    \n",
    "input -> satelite image with e.g. forest, house and river \n",
    "what can we do with this image \n",
    "- sigle label for the whole image\n",
    "- divide it into the region\n",
    "\n",
    "RAG - region adjencency graph\n",
    "Min SpaningTree -> randomly chop it off somewhere and find if regions are object\n",
    "when we fin the box, we always have to resize \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HaC\n",
    "- mering a bit more each step\n",
    "- creating kind a treee\n",
    "- dendrogram \n",
    "- each step megring things more similar as possible \n",
    "- we can draw a line over dendrogram and we can see how many clusters we will get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trunckated gaussian:\n",
    "    - gausian limited by bounding box \n",
    "    - by adding gausians together we create probability distribution \n",
    "- Flood fill\n",
    "- Mean shift !!!! \n",
    "    - allows us on more distinc clustering\n",
    "    \n",
    "EMGMM\n",
    "- fitting points into Gaussian distribution \n",
    "- multivariant normal distribution\n",
    "- measuring gausian in multi dim space \n",
    "0\n",
    "\n",
    "<img src=\"img/week721.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week722.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week724.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week726.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week727.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week728.png\" style=\"width:800px;height:300px;\">\n",
    "<img src=\"img/week729.png\" style=\"width:800px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
