{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient = direvative of weights/with respect of loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in supervised learrning: for machine learning algorithm we evaluate loss as function L(f(x1,w1),y) for each sample\n",
    "\n",
    "\n",
    "## unsupervised \n",
    "\n",
    "- unsuservised learning - 'Loss\" likechood of all the points under the whole density distribution, probability -> Loss(p) -> sum function-log(prob(x))\n",
    "\n",
    "EM\n",
    "sigma = normal distribution   \n",
    "$sum = 1/sqrt{2pi*sigma} * e^{-{x - distance_fromcentroid}^2/sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Triplet loss - each time imput, modify version of imput and solution\n",
    "- everytime we train on image with NN we loose the resolution, \n",
    "- filtering layers reduce the size of the image \n",
    "\n",
    "- in reduction layers we loosing info\n",
    "- sometimes in unsupervised problems we go through the NN and then again going through NN to come back to the input\n",
    "\n",
    "- proxy task of image compression,\n",
    "- going from the prediction layer back to the original image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back to supervised\n",
    "\n",
    "- we approximate loss using minibatches \n",
    "\n",
    "- we pick random 3 examples from the minibatch, they are approximation of the loss based on the minibatch\n",
    "\n",
    "- SGD - stochastic gradient descent, \n",
    "- since we coount on random minibatch, it can differ,\n",
    "- if loss function is shaky, we can stuck in the local minimum, when by using minibatch we kinda bounce, \n",
    "- ammound of how much 'webounce' it is size of learning rate \n",
    "\n",
    "\n",
    "## convolution \n",
    "- bigger grid, \n",
    "- we dont sample every single pixel, \n",
    "- we use every second pixel for sampling - DIALATED\n",
    "- each output is a function of a pixel, \n",
    "- pixel is an output of a region, it gives us some information about particulatr region -->RECEPTION FIELS \n",
    "\n",
    "- sometimes we can have 3 conv skipping 1,3,5 for each layer and then adding the results together --> they have the same size as an input \n",
    "- either concat or ADD to prodcue a proper output \n",
    "- we add them just at the final layer, but we can also add them on each step (less common)\n",
    "\n",
    "- (conv -> relu) x n for each layer, \n",
    "\n",
    "\n",
    "## sample CNN architecture \n",
    "\n",
    "- multiple blocks\n",
    "- reducing block want to reduce the size - mostly channels \n",
    "- maxPooling, each pixel after maxPool later, corespond to bigger ammount of pixels from the prev layer \n",
    "- usually patter is some building blocks -> reduce -> building blocks -> reduce \n",
    "- at the beginning usually enrichments -> more data and information we want to get \n",
    "- usually reduce layer, reducing image twice, usually ~5 downsampling layers,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## DAG NN\n",
    "- no cycle, graph which just goes forward, \n",
    "\n",
    "## ResNet \n",
    "- eriching and reducing blocks \n",
    "- activations capture a lot of information about the image \n",
    "- feature extraction unit \n",
    "\n",
    "## back bone \n",
    "- almost always using transfer learning\n",
    "- transfer learning without gradients (PyTorch), use network used for one task and then use it for another task \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
